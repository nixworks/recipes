{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of orthographic tokenization and grapheme segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steven Moran &lt;bambooforest@gmail.com&gt;\n",
    "\n",
    "The latest version of this [Jupyter notebook](http://jupyter.org/) is available at [https://github.com/unicode-cookbook/recipes/Basics](https://github.com/unicode-cookbook/recipes/Basics). \n",
    "\n",
    "This use case illustrates how to segment wordlist data using an orthography profile. Details about orthography profiles and more is available in the [Unicode Cookbook for Linguists](https://github.com/unicode-cookbook/cookbook).\n",
    "\n",
    "This recipes uses Python 3.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Python [segments](https://pypi.python.org/pypi/segments/) package to tokenize characters, graphemes and IPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from segments.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `characters` function will segment a string at Unicode code points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c ̂ h a ́ ɾ a ̃ ̌ c t ʼ ɛ ↗ ʐ ː | # k ͡ p\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "result = t.characters(\"ĉháɾã̌ctʼɛ↗ʐː| k͡p\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grapheme_clusters` function will segment text at the [Unicode Extended Grapheme Cluster](http://www.unicode.org/reports/tr18/tr18-19.html#Default_Grapheme_Clusters) boundaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĉ h á ɾ ã̌ c t ʼ ɛ ↗ ʐ ː | # k͡ p\n"
     ]
    }
   ],
   "source": [
    "result = t.grapheme_clusters(\"ĉháɾã̌ctʼɛ↗ʐː| k͡p\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grapheme_clusters` function is also the default segmentation algorithm for the `segments.Tokenizer`. It is useful when you encounter a text that you want to tokenize to identify orthographic or transcription elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĉ h á ɾ ã̌ c t ʼ ɛ ↗ ʐ ː | # k͡ p\n"
     ]
    }
   ],
   "source": [
    "result = t(\"ĉháɾã̌ctʼɛ↗ʐː| k͡p\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ipa` parameter forces grapheme segmentation for [IPA strings](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĉ h á ɾ ã̌ c tʼ ɛ ↗ ʐː | # k͡p\n"
     ]
    }
   ],
   "source": [
    "result = t(\"ĉháɾã̌ctʼɛ↗ʐː| k͡p\", ipa=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load an orthography profile and tokenize input string with it. In the `data` directory we've placed an example orthograpy profile. Let's have a look at it using `more` on the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1h\u001b=\r",
      "Grapheme        IPA     XSAMPA  COMMENT\r\n",
      "a       a       a\r\n",
      "aa      aː      a:\r\n",
      "b       b       b\r\n",
      "c       c       c\r\n",
      "ch      tʃ      tS\r\n",
      "-       NULL    NULL    \"comment with   tab\"\r\n",
      "on      õ       o~\r\n",
      "n       n       n\r\n",
      "ih      í       i_H\r\n",
      "inh     ĩ́       i~_H\r\n",
      "\r",
      "\u001b[K\u001b[?1l\u001b>"
     ]
    }
   ],
   "source": [
    "!more data/orthography-profile.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An orthograpy profile is tab-delimited UTF-8 text file. The first column must be labelled `Grapheme`. Each row in the `Grapheme` column specifies graphemes that may be found in the target input text's orthography. In this example, we provide additional columns `IPA` and `XSAMPA`, which are mappings from our graphemes to their IPA and XSAMPA transliterations. The final column `COMMENT` is for comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the orthography profile with our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from segments.tokenizer import Profile\n",
    "\n",
    "t = Tokenizer('data/orthography-profile.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's segment the graphemes in some input strings with our orthography profile. The output is segmented given the definition of graphemes in our orthography profile, e.g. we specified the sequence of two &lt;a a&gt; should be a single unit &lt;aa&gt;, and so should the sequences &lt;c h&gt;, &lt;o n&gt; and &lt;i h&gt;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa b ch on n - ih'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t('aabchonn-ih')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how can we tokenize input text into our orthograpic specification. We can also segment graphemes and transliterate them into other formats, which is useful when you have sources with different orthographies, but you want to be able to compare them using a single representation like IPA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aː b tʃ õ n í'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.transform(\"aabchonn-ih\", \"IPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a: b tS o~ n i_H'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.transform(\"aabchonn-ih\", \"XSAMPA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also useful to know which characters in your input string are not in your orthography profile. Use the function `find_missing_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa b ch on n - ih � � �'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.find_missing_characters(\"aa b ch on n - ih x y z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the default as the [Unicode replacement character \\ufffd](http://www.fileformat.info/info/unicode/char/fffd/index.htm). But you can simply change this by specifying the replacement character when you load the orthography profile with the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa b ch on n - ih ? ? ?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer('data/orthography-profile.tsv', errors_replace=lambda c: '?')\n",
    "t.find_missing_characters(\"aa b ch on n - ih x y z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa b ch on n - ih <x> <y> <z>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer('data/orthography-profile.tsv', errors_replace=lambda c: '<{0}>'.format(c))\n",
    "t.find_missing_characters(\"aa b ch on n - ih x y z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you want to create an initial orthography profile that also contains those graphemes x, y, z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grapheme\tfrequency\tmapping\n",
      " \t9\t \n",
      "a\t2\ta\n",
      "h\t2\th\n",
      "n\t2\tn\n",
      "b\t1\tb\n",
      "c\t1\tc\n",
      "o\t1\to\n",
      "-\t1\t-\n",
      "i\t1\ti\n",
      "x\t1\tx\n",
      "y\t1\ty\n",
      "z\t1\tz\n"
     ]
    }
   ],
   "source": [
    "profile = Profile.from_text(\"aa b ch on n - ih x y z\")\n",
    "print(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
